import{_ as e,c as n,o as a,ag as l}from"./chunks/framework.CDhRSyRB.js";const p=JSON.parse('{"title":"å‚è€ƒæ–‡çŒ®","description":"","frontmatter":{},"headers":[],"relativePath":"src/aircraft/reference.md","filePath":"src/aircraft/reference.md"}'),t={name:"src/aircraft/reference.md"};function o(r,i,s,c,d,m){return a(),n("div",null,i[0]||(i[0]=[l('<h1 id="å‚è€ƒæ–‡çŒ®" tabindex="-1">å‚è€ƒæ–‡çŒ® <a class="header-anchor" href="#å‚è€ƒæ–‡çŒ®" aria-label="Permalink to &quot;å‚è€ƒæ–‡çŒ®&quot;">â€‹</a></h1><ul><li>[1] Van Dijk T , De Wagter C , De Croon G C H E .Visual route following for tiny autonomous robots[J].Science Robotics, 2024, 9(92).DOI:10.1126/scirobotics.adk0310.</li><li>[2] Zhou X , Wang Z , Xu C ,et al.EGO-Planner: An ESDF-free Gradient-based Local Planner for Quadrotors[J]. 2020.DOI:10.1109/LRA.2020.3047728.</li><li>[3] Zhang W , Qi J , Wan P ,et al.An Easy-to-Use Airborne LiDAR Data Filtering Method Based on Cloth Simulation[J].Remote Sensing, 2016, 8(6):501.DOI:10.3390/rs8060501.</li><li>[4] Zheng C , Xu W , Zou Z ,et al.FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry[J]. 2024.DOI:10.1109/TRO.2024.3502198.</li><li>[5] Ma&#39;Arif A .Vision-Based Line Following Robot in Webots[J]. 2020.DOI:10.1109/FORTEI-ICEE50915.2020.9249943.</li><li>[6] Hao G , Lv Q , Huang Z ,et al.UAV Path Planning Based on Improved Artificial Potential Field Method[J].Aerospace (MDPI Publishing), 2023, 10(6).DOI:10.3390/aerospace10060562.</li><li>[7]è°¢è£åŸº.åŸºäºFPGAçš„åŒç›®è§†è§‰æƒ¯å¯¼é‡Œç¨‹è®¡è®¾è®¡ä¸å®ç°[D].ä¸œåŒ—å¤§å­¦,2021.</li><li>[8]Woodman OJ. 2007. An introduction to inertial navigation[R]. University of Cambridge Computer Laboratory, <a href="http://www.cl.cam.ac.uk/techreports/" target="_blank" rel="noreferrer">http://www.cl.cam.ac.uk/techreports/</a>. ISSN 1476-2986.</li><li>[9]He D , Xu W , Chen N ,et al.Pointâ€LIO: Robust Highâ€Bandwidth Light Detection and Ranging Inertial Odometry[J].Advanced Intelligent Systems (2640-4567), 2023, 5(7).DOI:10.1002/aisy.202200459.</li><li>[10]äºæŒ¯ä¸­,é—«ç»§å®,èµµæ°,ç­‰.æ”¹è¿›äººå·¥åŠ¿åœºæ³•çš„ç§»åŠ¨æœºå™¨äººè·¯å¾„è§„åˆ’[D].åŒ—äº¬äº¤é€šå¤§å­¦,2011.DOI:10.7666/d.y1229028.</li><li>[11] Chen X , Lbe T , Milioto A ,et al.OverlapNet: a siamese network for computing LiDAR scan similarity with applications to loop closing and localization[J].Autonomous Robots, 2022.DOI:10.1007/s10514-021-09999-0.</li><li>[12] Nava Y , Jensfelt P .Visual-LiDAR SLAM with loop closure[J]. 2018.</li><li>[13]Lang X, Chen C, Tang K, Ma Y, Lv J, Liu Y, Zuo X. 2023. Coco-LIC: Continuous-Time Tightly-Coupled LiDAR-Inertial-Camera Odometry Using Non-Uniform B-Spline[J]. IEEE Robotics and Automation Letters, 8(11): 7074.</li><li>[14]Boche S, Laina S B, Leutenegger S. Tightly-Coupled LiDAR-Visual-Inertial SLAM and Large-Scale Volumetric Occupancy Mapping[J]. arXiv preprint arXiv:2403.02280, 2024.</li><li>[15]é™ˆæµ©, æ¨æºä¼¦, èƒ¡ä¼Ÿå¥, ç­‰. åŸºäºå…¨æ™¯ç¯å¸¦æˆåƒçš„è¯­ä¹‰è§†è§‰é‡Œç¨‹è®¡[J]. Acta Optica Sinica, 2021, 41(22): 2215002.</li><li>[16]Schreiber K, Wunderlich T, Spilger P, et al. Emulating insect brains for neuromorphic navigation[J]. arXiv preprint arXiv:2401.00473, 2023.</li><li>[17]è®¸ä¿Šå‹‡,ç‹æ™¯å·,é™ˆå«ä¸œ.åŸºäºå…¨æ™¯è§†è§‰çš„ç§»åŠ¨æœºå™¨äººåŒæ­¥å®šä½ä¸åœ°å›¾åˆ›å»ºç ”ç©¶[J].æœºå™¨äºº, 2008, 30(4):9.DOI:10.3321/j.issn:1002-0446.2008.04.001.</li><li>[18]é«˜ç¿”,å¼ æ¶›,åˆ˜æ¯…,ç­‰.è§†è§‰SLAMåå››è®²:ä»ç†è®ºåˆ°å®è·µ[M].ç”µå­å·¥ä¸šå‡ºç‰ˆç¤¾,2019.</li><li>[19]Jerker B. Path Planning with Weighted Wall Regions using OctoMap[J]. 2018.</li><li>[20]Jia T, Yang E Y, Hsiao Y S, et al. Omu: A probabilistic 3d occupancy mapping accelerator for real-time octomap at the edge[J]. arXiv preprint arXiv:2205.03325, 2022.</li><li>[21]Hornung A, Wurm K M, Bennewitz M, et al. OctoMap: An efficient probabilistic 3D mapping framework based on octrees[J]. Autonomous robots, 2013, 34: 189-206.</li><li>[22]å…¨æƒ.å¤šæ—‹ç¿¼é£è¡Œå™¨â€”â€”æ§åˆ¶å®è·µçš„è¯•é‡‘çŸ³[J].ç³»ç»Ÿä¸æ§åˆ¶çºµæ¨ª,2019(1): 44-68.</li><li>[23]ç‹æ­£ç†™,é™ˆæ´‹,éƒ‘ç§€å¨Ÿ,ç­‰.é£æ‰°ä¸‹åŸºäºæ°”åŠ¨å‚æ•°ä¼°è®¡çš„å››æ—‹ç¿¼æ— äººæœºæ§åˆ¶[J].ä¿¡æ¯ä¸æ§åˆ¶, 2018, 47(06):663-670.DOI:10.13976/j.cnki.xk.2018.7490.</li><li>[24]å­™å«šæ†¶, æ¯•æ–‡è±ª, å¼ å®‰, ç­‰. åŸºäº MPC å’Œ ESO-DO çš„å››æ—‹ç¿¼è½¨è¿¹è·Ÿè¸ªæ§åˆ¶[J]. Command Control &amp; Simulation/Zhihui Kongzhi yu Fangzhen, 2024, 46(2).</li><li>[25]åº·æ™¶æ°, é»„æ”¿, æ–‡æµ©. åŸºäºåŒ…ç»œåˆ‡æ¢çš„å››æ—‹ç¿¼åŠæŒ‚æ— äººæœºé¿éšœè½¨è¿¹ä¼˜åŒ–ç ”ç©¶[J]. åŠ¨åŠ›å­¦ä¸æ§åˆ¶å­¦æŠ¥, 2021, 19(4): 73-80.</li><li>[26]ææ¡‚å­˜, æ–¹äºšæ¯œ, çºªè£ç¥, ç­‰. åŸºäºäºŒç»´æŒ¯é•œä¸ä½ç½®çµæ•æ¢æµ‹å™¨çš„é«˜ç²¾åº¦æ¿€å…‰è·Ÿè¸ªç³»ç»Ÿ[J]. Chinese Journal of Lasers, 2019, 46(7): 0704007.</li><li>[27]å´ä¸–å´‡,å»–é£,å´æ–‡å,ç­‰.åŸºäºé›†æˆå»ºæ¨¡æ–¹æ³•çš„å››æ—‹ç¿¼é€šç”¨æ§åˆ¶å™¨è®¾è®¡[J].ä¸­å›½ç§‘å­¦æŠ€æœ¯å¤§å­¦å­¦æŠ¥, 2020, 50(8):9.DOI:10.3969/j.issn.0253-2778.2020.08.007.</li><li>[28]é™ˆåŸ,ç¬¦ä¼Ÿæ°,åˆ˜å¨œ,ç­‰.åŸºäºæé›…æ™®è¯ºå¤«æŒ‡æ•°çš„å››æ—‹ç¿¼æ— äººé£è¡Œå™¨ç»“æ„å‚æ•°ä¼˜åŒ–ç¨³å®šæ€§åˆ†æ[J].åŠ¨åŠ›å­¦ä¸æ§åˆ¶å­¦æŠ¥, 2019, 17(6):11.DOI:10.6052/1672-6553-2019-044.</li><li>[29]é«˜ç¦æµ·,èƒ¡ç››æ–Œ,ç‹æ™¨æ‚¦.åŸºäºå†…å¤–ç¯çš„å››æ—‹ç¿¼é£è¡Œå™¨çš„åŒæ¨¡ç³Šæ»‘æ¨¡æ§åˆ¶[J].æ™ºèƒ½è®¡ç®—æœºä¸åº”ç”¨, 2020.DOI:10.3969/j.issn.2095-2163.2020.07.037.</li><li>[30]æ›¹ç¾ä¼š,é²œæ–Œ,å¼ æ—­,ç­‰.åŸºäºè§†è§‰çš„å››æ—‹ç¿¼æ— äººæœºè‡ªä¸»å®šä½ä¸æ§åˆ¶ç³»ç»Ÿ[J].ä¿¡æ¯ä¸æ§åˆ¶, 2015, 44(2):8.DOI:10.13976/j.cnki.xk.2015.0190.</li><li>[31]å®‹å¿—å¼ºã€æ–¹æ­¦ã€åˆ˜å­èµµ.åŸºäºPDæ§åˆ¶çš„å››æ—‹ç¿¼æ— äººæœºç€é™†æ§åˆ¶ç ”ç©¶[J].è®¡ç®—æœºåº”ç”¨ä¸è½¯ä»¶, 2020, 37(9):4.DOI:CNKI:SUN:JYRJ.0.2020-09-055.</li><li>[32]å‘¨è±ª,æ˜“æ˜ è.åŸºäºSimscapeçš„å››æ—‹ç¿¼æ— äººæœºç‰©ç†æ¨¡å‹ä»¿çœŸ[J].å»ºæ¨¡ä¸ä»¿çœŸ, 2022, 11(5):10.DOI:10.12677/MOS.2022.115115.</li><li>[33]è®¸å–†.åŸºäºSMCçš„å››æ—‹ç¿¼æ— äººæœºæŠ—é£æ‰°ç ”ç©¶[J].ç”µå…‰ä¸æ§åˆ¶, 2017, 24(1):5.DOI:10.3969/j.issn.1671-637X.2017.01.016.</li><li>[34]é™ˆè¶…,æ®µçº³,å¾æ­¢æ”¿.å…·æœ‰è¾“å…¥æ­»åŒºä¸æ‰°åŠ¨çš„å››æ—‹ç¿¼æ— äººæœºè‡ªæŠ—æ‰°æ§åˆ¶[J].ä¿¡æ¯ä¸æ§åˆ¶, 2023, 52(3):326-333.DOI:10.13976/j.cnki.xk.2023.2139.</li><li>[35]åˆ˜å½¦ä¼Ÿ,åˆ˜ä¸‰å¨ƒ,ç‹ææ¢¦,ç­‰.å¯å€¾è½¬å˜å½¢å››æ—‹ç¿¼é£è¡Œå™¨å»ºæ¨¡ä¸é£è¡Œä»¿çœŸ[J].æœºæ¢°ç§‘å­¦ä¸æŠ€æœ¯, 2020, 39(4):6.DOI:10.13433/j.cnki.1003-8728.20200004.</li><li>[36]Qiangweizhai Ma,Hongyu Wang,Hui Hong,ç­‰.æ˜†è™«æœºå™¨æ··åˆç³»ç»Ÿ: ä»è‡ªç”±çŠ¶æ€æ§åˆ¶åˆ°è‡ªä¸»æ™ºèƒ½è°ƒæ§[J].Chinese Science Bulletin, 2023.DOI:10.1360/tb-2023-0186.</li><li>[37]é»„è¿ª,é™†ä¼Ÿæ°‘,åº”å½¬.æ¨¡å‹ä¸ç¡®å®šå’ŒæœªçŸ¥æ‰°åŠ¨ä¸‹å››æ—‹ç¿¼æ— äººæœºä½ç½®ä¸å§¿æ€æ§åˆ¶[J].èˆªå¤©æ§åˆ¶, 2024, 42(4):22-28.</li><li>[38]äºæ–‡å¦,æ¨å¤æ—.å››æ—‹ç¿¼æ— äººæœºä¸²çº§æ¨¡ç³Šè‡ªé€‚åº”PIDæ§åˆ¶ç³»ç»Ÿè®¾è®¡[J].æœºæ¢°è®¾è®¡ä¸åˆ¶é€ , 2019(1):5.DOI:CNKI:SUN:JSYZ.0.2019-01-060.</li><li>[39]æä¿ŠèŠ³, æå³°y, å‰æœˆè¾‰, ç­‰. å››æ—‹ç¿¼æ— äººæœºè½¨è¿¹ç¨³å®šè·Ÿè¸ªæ§åˆ¶[J].æ§åˆ¶ä¸å†³ç­–, 2020, 35(2): 349-356.</li><li>[40]æ¨åˆ™å…, æçŒ›, å¼ å…¨. å››æ—‹ç¿¼æ— äººæœºæ§åˆ¶ç³»ç»Ÿè®¾è®¡ä¸å®ç°[J]. ç§‘æŠ€åˆ›æ–°ä¸åº”ç”¨, 2018, 8(34): 105-106.</li><li>[41]éƒä¼Ÿ, é²œæ–Œ. å››æ—‹ç¿¼æ— äººæœºå§¿æ€ç³»ç»Ÿçš„éçº¿æ€§å®¹é”™æ§åˆ¶è®¾è®¡[J]. æ§åˆ¶ç†è®ºä¸åº”ç”¨, 2015, 32(11): 1457-1463.</li><li>[42]ç‹ä¿Šå½¦, æ¨èŸæ†­, ç‹è¿ªèŒœ, ç­‰. å››æ—‹ç¿¼æ— äººæœºæ¡¨å°–ç¼ºæŸæ•…éšœå»ºæ¨¡[J]. Journal of Aerospace Science and Technology, 2022, 10: 15.</li><li>[43]è‚åšæ–‡, é©¬å®ç»ª, ç‹å‰‘, ç­‰. å¾®å°å‹å››æ—‹ç¿¼é£è¡Œå™¨çš„ç ”ç©¶ç°çŠ¶ä¸å…³é”®æŠ€æœ¯[D]. , 2007.</li><li>[44]éƒ­é›·, ä½™ç¿”, å¼ éœ„, ç­‰. æ— äººæœºå®‰å…¨æ§åˆ¶ç³»ç»ŸæŠ€æœ¯: è¿›å±•ä¸å±•æœ›[J]. ä¸­å›½ç§‘å­¦: ä¿¡æ¯ç§‘å­¦, 2020, 50(2): 184-194.</li><li>[45]ç‹è¯—ç« , é²œæ–Œ, æ¨æ£®. æ— äººæœºåŠæŒ‚é£è¡Œç³»ç»Ÿçš„å‡æ‘†æ§åˆ¶è®¾è®¡[J]. è‡ªåŠ¨åŒ–å­¦æŠ¥, 2018, 44(10): 1771-1780.</li><li>[46]å¼ æµ·å†›, é™ˆæ˜ è¾‰. æ™ºèƒ½å‹å››æ—‹ç¿¼æ— äººæœºé£æ§ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°[J]. è®¡ç®—æœºåº”ç”¨ä¸è½¯ä»¶, 2019, 36(4): 73-78.</li><li>[47]å­£æ™“æ˜,æ–‡æ€€æµ·. è‡ªé€‚åº”ç¥ç»ç½‘ç»œå››æ—‹ç¿¼æ— äººæœºæœ‰é™æ—¶é—´è½¨è¿¹è·Ÿè¸ªæ§åˆ¶[J]. æ™ºèƒ½ç³»ç»Ÿå­¦æŠ¥, 2022, 17(3): 540-546.</li><li>[48]Sathyabama Institute of Science and Technology. 2021. Fundamentals of Artificial Neural Networks - SEC1609[Z]. Sathyabama Institute of Science and Technology. Available at: <a href="https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SEC1609.pdf" target="_blank" rel="noreferrer">https://sist.sathyabama.ac.in/sist_coursematerial/uploads/SEC1609.pdf</a>. [Accessed on: 2024-12-24].</li><li>[49]Jain A K, Mao J, Mohiuddin K M. Artificial neural networks: A tutorial[J]. Computer, 1996, 29(3): 31-44.</li><li>[50]Ravichandran S. Artificial Neural Network Modelling and its Applications in agriculture[J].</li><li>[51]Dastres R, Soori M. Artificial Neural Network Systems[J]. International Journal of Imaging and Robotics (IJIR), 2021, 21(2): 13-25.</li><li>[52]Marini F. Artificial Neural Networks[M]. Rome: Dept. Chemistry, University of Rome â€œLa Sapienzaâ€, Italy, 2022. * [2024-12-24]. Available from: <a href="https://www.sensorfint.eu/wp-content/uploads/2022/11/ANN.pdf" target="_blank" rel="noreferrer">https://www.sensorfint.eu/wp-content/uploads/2022/11/ANN.pdf</a>.</li><li>[53]Qian C, Jia Y, Wang Z, et al. Autonomous aeroamphibious invisibility cloak with stochastic-evolution learning[J]. Advanced Photonics, 2024, 6(1): 016001-016001.</li><li>[54]Hritsev A. The ANN book[M]. 2004. Available from: <a href="http://www.pdg.cnb.uam.es/cursos/Complutense/Complutense2004/pages/12_NeuralNetworks/Hritsev_The_ANN_Book.pdf" target="_blank" rel="noreferrer">http://www.pdg.cnb.uam.es/cursos/Complutense/Complutense2004/pages/12_NeuralNetworks/Hritsev_The_ANN_Book.pdf</a>. [Accessed on 2024-12-24].</li><li>[55]Ribeiro M I. Kalman and extended kalman filters: Concept, derivation and properties[J]. Institute for Systems and Robotics, 2004, 43(46): 3736-3741.</li><li>[56]Gu B, Feng J, Song Z. Looming detection in complex dynamic visual scenes by interneuronal coordination of motion and feature pathways[J]. Advanced Intelligent Systems, 2024, 6(9): 2400198.</li><li>[57]Elgammal A. CS536: Machine Learning Artificial Neural Networks[EB/OL]. (2005)* [2024-12-24]. <a href="https://people.cs.rutgers.edu/~elgammal/classes/cs536/lectures/ANN.pdf" target="_blank" rel="noreferrer">https://people.cs.rutgers.edu/~elgammal/classes/cs536/lectures/ANN.pdf</a>.</li><li>[58]æ²ˆæ¨æ¨,æ¨å¿ ,å¼ ç¿”,ç­‰.ä¸€ç§å€¾è½¬å››æ—‹ç¿¼æ— äººæœºåŠå…¶è¿‡æ¸¡æ®µå§¿æ€æ§åˆ¶[J].å…µå·¥è‡ªåŠ¨åŒ–, 2018, 37(3):6.DOI:10.7690/bgzdh.2018.03.018.</li><li>[59]ä¸å°‘å®¾,è‚–é•¿è¯—,åˆ˜é‡‘æ ¹,ç­‰.Xå‹å››æ—‹ç¿¼æ— äººæœºå»ºæ¨¡åŠå››å…ƒæ•°æ§åˆ¶[J].ç³»ç»Ÿä»¿çœŸå­¦æŠ¥, 2015(12):6.DOI:CNKI:SUN:XTFZ.0.2015-12-027.</li><li>[60]Terejanu G A. Extended kalman filter tutorial[J]. University at Buffalo, 2008, 27.</li><li>[61]Collins L T. The case for emulating insect brains using anatomical â€œwiring diagramsâ€ equipped with biophysical models of neuronal activity[J]. Biological Cybernetics, 2019, 113(5): 465-474.</li><li>[62]æ¨è‹, è°·å‹‡éœ, å°¹ä¸¹å¦®, ç­‰. åŸºäºä¸‰ç»´é‡å»ºæŠ€æœ¯çš„æ˜†è™«è„‘ç»“æ„ç ”ç©¶è¿›å±•[J]. Computer Science and Application, 2020, 10: 1507.</li><li>[63]å­™åˆš,æˆ¿å²©.æ˜†è™«ä½“è¡¨å¤åˆæµ¸æ¶¦æ€§ä¸ä»¿ç”Ÿè®¾è®¡ ç”Ÿç‰©ç§‘å­¦[M].æ¸…åå¤§å­¦å‡ºç‰ˆç¤¾,2022: 1-11.</li><li>[64]é™ˆé›·,å¼ ç‘¶ä¼Ÿ,ç‹ç¡•,ç­‰.FPGAä¸‰æ¨¡å†—ä½™å·¥å…·çš„å…³é”®æŠ€æœ¯ä¸å‘å±•[J].ç”µå­ä¸ä¿¡æ¯å­¦æŠ¥, 2022, 44(6):15.</li><li>[65]æç™»é™,èŒƒå®ˆæ–‡.åŸºäºFPGAçš„åŒCPUå®¹é”™æ§åˆ¶å™¨è®¾è®¡[J].è®¡ç®—æœºå·¥ç¨‹, 2010(2):3.DOI:10.3969/j.issn.1000-3428.2010.02.084.</li><li>[66]é»„å½±,å¼ æ˜¥å…ƒ,åˆ˜ä¸œ.åŸºäºFPGAåŒæœºå®¹é”™ç³»ç»Ÿçš„è®¾è®¡ä¸å®ç°[J].æ·±åœ³å¤§å­¦å­¦æŠ¥ï¼šç†å·¥ç‰ˆ, 2006, 23(2):5.DOI:10.3969/j.issn.1000-2618.2006.02.005.</li><li>[67]ææ°,æ²ˆé”.ç©ºé—´è®¡ç®—æœºå†—ä½™æ¶æ„å¯é æ€§åˆ†ææ¯”è¾ƒ[J].æ·±ç©ºæ¢æµ‹å­¦æŠ¥, 2018, 5(6):7.DOI:CNKI:SUN:SKTC.0.2018-06-011.</li><li>[68]Jiao Q, Chen D, Huang Y, Li Y, Shen Y. From Training-Free to Adaptive: Empirical Insights into MLLMs&#39; Understanding of Detection Information[J/OL]. (2024)* [2024-12-24]. arXiv: 2401.17981. <a href="http://arxiv.org/abs/2401.17981" target="_blank" rel="noreferrer">http://arxiv.org/abs/2401.17981</a>.</li><li>[69]å¼ å°‘å†›,å¾ç†™å¹³.360Â°é«˜é˜¶éçƒé¢åå°„å¼å…¨æ™¯é•œå¤´è®¾è®¡[J].å…‰å­¦ç²¾å¯†å·¥ç¨‹, 2018, 26(8):8.DOI:CNKI:SUN:GXJM.0.2018-08-019.</li><li>[70]æœ±å¨,éŸ©å·¨å³°,éƒ‘é›…ç¾½,ç­‰.åŸºäºDSPçš„å…¨æ™¯è§†é¢‘å¤šç›®æ ‡å®æ—¶æ£€æµ‹[J].å…‰ç”µå·¥ç¨‹, 2014, 41(5).DOI:10.3969/j.issn.1003-501X.2014.05.012.</li><li>[71]å‘¨æ¸æ–Œ.åŸºäºFPGA+DSPçš„æ™ºèƒ½è½¦å…¨æ™¯è§†è§‰ç³»ç»Ÿ[J].ç”µå­æŠ€æœ¯åº”ç”¨, 2011, 37(3):4.DOI:10.3969/j.issn.0258-7998.2011.03.018.</li><li>[72]ææ–‡æ¶›ã€å¼ å²©ã€è«é”¦ç§‹ã€æå½¦æ˜ã€åˆ˜æˆè‰¯.åŸºäºæ”¹è¿›YOLOv3-tinyçš„ç”°é—´è¡Œäººä¸å†œæœºéšœç¢ç‰©æ£€æµ‹[J].å†œä¸šæœºæ¢°å­¦æŠ¥, 2020, 51(S01):9.</li><li>[73]å…¨çº¢è‰³,ç‹é•¿æ³¢,æ—ä¿Šéš½.åŸºäºè§†è§‰çš„å¢å¼ºç°å®æŠ€æœ¯ç ”ç©¶ç»¼è¿°[J].æœºå™¨äºº, 2008, 30(4):6.DOI:10.3321/j.issn:1002-0446.2008.04.014.</li><li>[74]åˆ˜æµ·é¾™,å¼ æ™ºæ ‹,æ¨åœåœ.åŸºäºåŒç›®é±¼çœ¼æ‘„åƒå¤´å…¨æ™¯å›¾ç‰‡çš„å¯»äººç³»ç»Ÿ[J].æ™ºèƒ½è®¡ç®—æœºä¸åº”ç”¨, 2021, 011(008)ğŸ˜›.15-18,22.</li><li>[75]ç‹è¾æ™“,æè´º,å°šä¿Šæ°.åŸºäºè™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®çš„æ•™è‚²æ¸¸æˆåº”ç”¨åŠå‘å±•å‰æ™¯[J].ä¸­å›½ç”µåŒ–æ•™è‚², 2017(8):9.DOI:10.3969/j.issn.1006-9860.2017.08.016.</li><li>[76]è´¾å®‡èˆª,å‘¨æ¢¦é¾™,é™ˆåŸºèŒ—,ç­‰.åŸºäºè™šæ‹Ÿç°å®æŠ€æœ¯çš„äººæœºäº¤äº’åœ¨å†›äº‹æ¼”ç»ƒä¸­çš„åº”ç”¨å‘å±•è¶‹åŠ¿[J].äººå·¥æ™ºèƒ½ä¸æœºå™¨äººç ”ç©¶, 2022, 11(4):8.DOI:10.12677/AIRR.2022.114035.</li><li>[77]æçº¢æ³¢,å´æ¸,ç‹ç»ª,ç­‰.ä¸€ç§é‡‡ç”¨å¢å¼ºç°å®æŠ€æœ¯çš„å•å…µè½»æ­¦å™¨å°„å‡»è®­ç»ƒçš„æ–¹æ³•å’Œç³»ç»Ÿ:CN201210186926.5[P].CN102735100A* [2024-12-24].</li><li>[78]æœ±æµ·æ»¨,å¼ è¿œå¥,å‘¨äº®,ç­‰.åŒæ³¢æ®µ/åŒè§†åœºå…¨æ™¯èˆªç©ºç›¸æœºå…‰å­¦ç³»ç»Ÿè®¾è®¡[J].å…‰å­¦ä»ªå™¨, 2017, 039(002):48-52.</li><li>[79]å¾è¾‰,ç¥ç‰å,ç”„å½¤,ç­‰.æ·±åº¦ç¥ç»ç½‘ç»œå›¾åƒè¯­ä¹‰åˆ†å‰²æ–¹æ³•ç»¼è¿°[J].è®¡ç®—æœºç§‘å­¦ä¸æ¢ç´¢, 2021, 15(1):13.DOI:10.3778/j.issn.1673-9418.2004039.</li><li>[80]Howard A , Sandler M , Chen B ,et al.Searching for MobileNetV3[C]//2019 IEEE/CVF International Conference on Computer Vision (ICCV).IEEE, 2020.DOI:10.1109/ICCV.2019.00140.</li><li>[81]Du Y , Li C , Guo R ,et al.PP-OCR: A Practical Ultra Lightweight OCR System[J]. 2020.DOI:10.48550/arXiv.2009.09941.</li><li>[82]Li C, Liu W, Guo R, et al. PP-OCRv3: More attempts for the improvement of ultra lightweight OCR system[J]. arXiv preprint arXiv:2206.03001, 2022.</li><li>[83]æ±Ÿäº‘å³°,ç½—æ•,ä½•çº¢æ˜Ÿ,ç­‰.é±¼çœ¼é•œå¤´çš„ç ”ç©¶è¿›å±•åŠåº”ç”¨[J].çº¢å¤–æŠ€æœ¯, 2023, 45(4):342-351.</li><li>[84]æ—æ³‰,æç£Š,æ–¹ä¸­å,ç­‰.å…¨æ™¯æ‘„åƒæœºçš„åŸç†ä¸è¿›å±•[J].è‡ªç„¶æ‚å¿—, 2017, 39(2):6.DOI:10.3969/j.issn.0253-9608.2017.02.007.</li><li>[85]ç‹æ™“äº®,ç‹æ‰¿ç¥¥,ä½•ç‚œç¨.åˆ©ç”¨é£æ§ç‰¹å¾çš„å››æ—‹ç¿¼æ— äººæœºé›·è¾¾å›æ³¢ä¿¡å·ä»¿çœŸ[J].ä¿¡å·å¤„ç†, 2021(037-002).</li><li>[86]é¡¾æœˆæ¸…,é™¶å®ç¥º.éšèº«æŠ€æœ¯åœ¨é£æœºä¸Šçš„åº”ç”¨[J].ç‰©ç†, 1996, 25(11):6.DOI:JournalArticle/5aedfce4c095d710d410ba80.</li><li>[87]DÃ­az IC, Fogg M, Alvarez L, Dieguez M. Study on active noise cancellation[EB/OL]. (2021)* [2024-12-24]. <a href="https://github.com/iancraz/ANC-Implementation/blob/master/README.md" target="_blank" rel="noreferrer">https://github.com/iancraz/ANC-Implementation/blob/master/README.md</a>.</li><li>[88]æ—¶æ™¨å…‰,è‘£ç’Ÿ,å‘¨å»ºæ±Ÿ,ç­‰.é£è¡Œå™¨å°„é¢‘éšèº«æŠ€æœ¯ç ”ç©¶ç»¼è¿°[J].ç³»ç»Ÿå·¥ç¨‹ä¸ç”µå­æŠ€æœ¯, 2021, 43(6):16.</li><li>[89]ä¹”æ²»å†›,æ½˜ç»ªè¶…,ä½•å‹‡,et al.é«˜åŠŸç‡ç”µç£è„‰å†²å¯¹æ— äººé£è¡Œå™¨çš„æ¯ä¼¤ç ´å[J]. 2017.</li><li>[90]ä¸­å›½æ°‘ç”¨èˆªç©ºå±€. MH/T 6126â€”2022 åŸå¸‚åœºæ™¯ç‰©æµç”µåŠ¨å¤šæ—‹ç¿¼æ— äººé©¾é©¶èˆªç©ºå™¨ï¼ˆè½»å°å‹ï¼‰ç³»ç»ŸæŠ€æœ¯è¦æ±‚[S]. åŒ—äº¬ï¼šä¸­å›½æ°‘ç”¨èˆªç©ºå±€ï¼Œ2022. å‘å¸ƒæ—¥æœŸï¼š2022-03-07ï¼Œå®æ–½æ—¥æœŸï¼š2022-04-01.</li><li>[91]è–›ç§‘å³°.æ— äººæœºAIæ‰“å‡»çŠ¯ç½ªæå‡å…¬å®‰å·¥ä½œç°ä»£åŒ–æ°´å¹³è·¯å¾„ç ”ç©¶[J].å…¬å®‰ç ”ç©¶, 2024(9).</li><li>[92]æ·±åœ³å¸‚æ ‡å‡†æŠ€æœ¯ç ”ç©¶é™¢. å¤šæ—‹ç¿¼é£è¡Œå™¨é£è¡Œå“è´¨æ ‡å‡†[R]. æ·±åœ³ï¼šæ·±åœ³å¸‚æ ‡å‡†æŠ€æœ¯ç ”ç©¶é™¢ï¼Œ2020.09.</li><li>[93]ä¸­å›½æ°‘ç”¨èˆªç©ºå±€èˆªç©ºå™¨é€‚èˆªå®¡å®šå¸. æ°‘ç”¨æ— äººé©¾é©¶èˆªç©ºå™¨ç³»ç»Ÿé€‚èˆªå®‰å…¨è¯„å®šæŒ‡å—[Z]. æ°‘èˆªé€‚å‡½ã€”2024ã€•5å·ï¼ŒAC-92-AA-2024-01. åŒ—äº¬ï¼šä¸­å›½æ°‘ç”¨èˆªç©ºå±€èˆªç©ºå™¨é€‚èˆªå®¡å®šå¸ï¼Œ2024-02-05.</li><li>[94]äºå‘¨é”‹,ç‹æƒ æ—.æ— äººæœºå…‰ç”µå¯¹æŠ—æŠ€æœ¯ç ”ç©¶[J].åº”ç”¨å…‰å­¦, 2021, 42(3):6.DOI:10.5768/JAO202142.0301001.</li><li>[95]å¼ äºšæ–Œï¼Œå¶èŒ‚ï¼Œç‹ä¼šæ³•ï¼Œé™ˆäº®ç¥¥. æ— äººæœºæŠ€æœ¯åœ¨å†›äº‹é¢†åŸŸçš„è¿ç”¨ä¸å‘å±•[J]. è‰ºæœ¯ä¸è®¾è®¡ï¼Œ2023, 23(11): 215-217.</li><li>[96]é™ˆè€€ç’,è‚–ç®é¾„.æ— äººæœºåœ¨è­¦åŠ¡å·¥ä½œä¸­çš„åº”ç”¨ç ”ç©¶[J].äººå·¥æ™ºèƒ½ä¸æœºå™¨äººç ”ç©¶, 2021, 10(4):7.DOI:10.12677/AIRR.2021.104029.</li><li>[97]å¼ å¤©èˆª,ç™½é‡‘å¹³.æ—‹ç¿¼å¼æ— äººæœºçš„å‘å±•å’Œè¶‹åŠ¿[J].artificial intelligence&amp;robotics research, 2013, 02:16-23.DOI:10.12677/AIRR.2013.21003.</li><li>[98]Vanneste S , Bellekens B , Weyn M .3DVFH+: Real-Time Three-Dimensional Obstacle Avoidance Using an Octomap[J]. 2014.</li><li>[99]ç‹æ¢¦æ¡¥,åˆ˜äºŒæ—.åŸºäºæ”¹è¿›åŒå‘RRTç®—æ³•çš„æ— äººæœºèˆªè¿¹è§„åˆ’(è‹±æ–‡)[J/OL].Journal of Measurement Science and Instrumentation,1-9* [2024-11-16].<a href="http://kns.cnki.net/kcms/detail/14.1357.th.20241114.0958.002.html" target="_blank" rel="noreferrer">http://kns.cnki.net/kcms/detail/14.1357.th.20241114.0958.002.html</a>.</li><li>[100]å¼ é¡º,è°¢ä¹ å,é™ˆå®šå¹³.åŸºäºæ”¹è¿›RRT-Connectçš„æ— äººæœºèˆªè¿¹è§„åˆ’ç®—æ³•[J].ä¼ æ„Ÿå™¨ä¸å¾®ç³»ç»Ÿ,2020,39(12):146-148+156.DOI:10.13873/J.1000-9787(2020)12-0146-03.</li><li>[101]è®¸äº‘é¹.åŸºäºæ·±åº¦å­¦ä¹ çš„æ— äººæœºå¤šåœºæ™¯æ„ŸçŸ¥ä¸è·¯å¾„è§„åˆ’æŠ€æœ¯ç ”ç©¶[D].åŒ—äº¬é‚®ç”µå¤§å­¦,2024.DOI:10.26969/d.cnki.gbydu.2024.000098.</li><li>[102] Liu Fangcen;Gao Chenqiang;Chen Fang;Meng Deyu;Zuo Wangmeng;Gao Xinbo. Infrared Small and Dim Target Detection with Transformer under Complex Backgrounds.[J].IEEE transactions on image processing : a publication of the IEEE Signal Processing Society.2023</li><li>[103] Su Jinya;Zhu Xiaoyong;Li Shihua;Chen Wen-Hua. AI meets UAVs: A survey on AI empowered UAV perception systems for precision agriculture[J].Neurocomputing.2023</li><li>[104]åˆ˜å¼º,èµ–èŠå…°,å†¯ æ™¶,ç­‰.æ”¹è¿›çš„åŒå‘RRTè·¯å¾„è§„åˆ’ç®—æ³•ç ”ç©¶[J].äººå·¥æ™ºèƒ½ä¸æœºå™¨äººç ”ç©¶, 2022, 11(4):8.DOI:10.12677/AIRR.2022.114044.</li><li>[105]ä¿å®¬,é™ˆè°‹,é›å¯å—.åŸºäºæ”¹è¿›RRT*ç®—æ³•çš„æ— äººæœºå¾€è¿”èˆªè¿¹è§„åˆ’[J].ä¸­å›½ç§‘å­¦:æŠ€æœ¯ç§‘å­¦, 2023, 53(11):1911-1921.</li><li>[106]å§œé¦™èŠ,é»„ç‚³å¾·,æ¨æ½‡æ´.åº”ç”¨äºæ— äººæœºå…¨å±€èˆªè¿¹è§„åˆ’çš„æ”¹è¿›åŒå‘RRTsç®—æ³•[J].æœºæ¢°ç§‘å­¦ä¸æŠ€æœ¯, 2024, 43(5):897-903.</li><li>[107]ç§¦å¤©æ˜Š, è¾›ç»æ°. åŸºäºæ”¹è¿›çš„åŒå‘å¿«é€Ÿéšæœºæœç´¢æ ‘çš„è·¯å¾„è§„åˆ’ç®—æ³•[J]. ç‰©ç†å­¦æŠ¥: ä¼šè®®ç³»åˆ—, 2022, 2396(1): 012055.</li><li>[108]ç†Šé™, æ®µå°å¤. åŸºäºæ”¹è¿›åŠ¨æ€æ­¥é•¿RRTç®—æ³•çš„æ— äººæœºè·¯å¾„è§„åˆ’[J]. ç‰©ç†å­¦æŠ¥: ä¼šè®®ç³»åˆ—, 2021, 1983(1): 012034.</li><li>[109]Vaswani A. Attention is all you need[J]. Advances in Neural Information Processing Systems, 2017.</li><li>[110]Gu A, Dao T. Mamba: Linear-time sequence modeling with selective state spaces[J]. arXiv preprint arXiv:2312.00752, 2023.</li><li>[111]Xiao T, Zhu J. Introduction to Transformers: an NLP Perspective[J]. arXiv preprint arXiv:2311.17633, 2023.</li><li>[112]Turner R E. An introduction to transformers[J]. arXiv preprint arXiv:2304.10557, 2023.</li><li>[113]ä¿æ—­è±ª. æ— äººæœºåœ¨æ¶ˆé˜²ç­ç«æ•‘æ´ä¸­çš„åº”ç”¨[J]. å»ºç­‘å‘å±•, 2021, 5(1): 39-40. DOI:10.12238/bd.v5i1.3646.</li><li>[114]å¤§ç–†åˆ›æ–°å…¬å¸.å†œä¸šæ— äººæœºè¡Œä¸šç™½çš®ä¹¦.</li><li>[115]å¼ å¿—äº‘,æé•¿è´º.æ— äººæœºæŠ€æœ¯åœ¨ç°ä»£å†œä¸šä¸­çš„åº”ç”¨[J].å†œä¸šå·¥ç¨‹, 2016, 6(4):3.DOI:10.3969/j.issn.2095-1795.2016.04.008.</li><li>[116]è’‹ä¸‰ç”Ÿ,éƒ­ è¾‰,ç‹ å°š,ç­‰.æ— äººæœºå†œä¸šæ¤ä¿åº”ç”¨ç ”ç©¶æ–°è¿›å±•[J].å†œä¸šç§‘å­¦, 2022, 12(11):7.DOI:10.12677/HJAS.2022.1211157.</li><li>[117]åˆ˜åŒå¥‡.è¯•ææˆ‘å›½æ— äººæœºçš„å‘å±•ç°çŠ¶åŠå±•æœ›[J].ç§‘æŠ€é£,2019,(12):204.DOI:10.19392/j.cnki.1671-7341.201912176.</li><li>[118]é»„é™,å¼ çš“ç³.æ— äººæœºåŠ¨åŠ›æŠ€æœ¯å‘å±•ç°çŠ¶ä¸å±•æœ›[J].ä¿¡æ¯æŠ€æœ¯ä¸ä¿¡æ¯åŒ–,2019,(12):202-204.</li><li>[119]å‘¨å­æ ‹,é™ˆè‡³å¤,èµµå¿—ä½³.å››æ—‹ç¿¼æ— äººæœºé£æ§ç®—æ³•ç»¼è¿°[J].ç½‘ç»œå®‰å…¨æŠ€æœ¯ä¸åº”ç”¨, 2019(9):4.DOI:CNKI:SUN:WLAQ.0.2019-09-019.</li><li>[120]æ®µä½³ä¿Š,ç½—å†¬ç‘¾,æ¨é‡‘é¾™.æ— äººæœºå‘å±•å†å²,ç°çŠ¶åŠæœªæ¥å‘å±•è¶‹åŠ¿[J].ç§‘æ•™æ–‡åŒ–.2021(2):48-51.</li><li>[121]æ¨ç»­æ¥, è¢å¸…å¸…, æ¨æ–‡é™, ç­‰. é”‚ç¦»å­åŠ¨åŠ›ç”µæ± èƒ½é‡å¯†åº¦ç‰¹æ€§ç ”ç©¶è¿›å±•[J]. æœºæ¢°å·¥ç¨‹å­¦æŠ¥, 2023, 59(6): 239-254.</li><li>[122]éƒä¸ºæ°‘,é©¬å«å›½,èµµå“è,ç­‰.ç³»ç•™æ— äººæœºç³»ç»Ÿç ”ç©¶[J].ç”µä¿¡å¿«æŠ¥,2022,(11):1-6.</li><li>[123]éš†èˆª.æ— äººæœºå®¤å†…è‡ªä¸»é£è¡Œé¿éšœç ”ç©¶[D].è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦,2021.DOI:10.27389/d.cnki.gxadu.2021.003145.</li><li>[124]é€šç”¨èˆªç©ºè£…å¤‡åˆ›æ–°åº”ç”¨å®æ–½æ–¹æ¡ˆ(2024ä¸€2030å¹´)[J].ä¸­å›½å†›è½¬æ°‘,2024,(22):6-8.</li><li>[125]æ¨å­æˆ¿,è¢å®¶æ”¿,å¾æˆ,ç­‰.åŸºäºYOLOç³»åˆ—çš„ç›®æ ‡æ£€æµ‹ç ”ç©¶è¿›å±•ç»¼è¿°[C]<a href="//xn--fiq7va501a70bq0jxnpz6g56l468aea178qbxj1rbx88g.xn--2024-ob5fhgtzl9aag536d70bg1gmfl75cxuu8mifa90dma946qkpbo21b71av7yxhs255aeaq4037bmycoz1boa670bpa3654im1ap481a.xn--1lqa14gqzf0qab00ox7t45eeqa8bu02bykaz30a9zz7l9a5d7afwxtt7erttb" target="_blank" rel="noreferrer">//ä¸­å›½è®¡ç®—æœºç”¨æˆ·åä¼šç½‘ç»œåº”ç”¨åˆ†ä¼š.ä¸­å›½è®¡ç®—æœºç”¨æˆ·åä¼šç½‘ç»œåº”ç”¨åˆ†ä¼š2024å¹´ç¬¬äºŒåå…«å±Šç½‘ç»œæ–°æŠ€æœ¯ä¸åº”ç”¨å¹´ä¼šè®ºæ–‡é›†.åŒ—äº¬è”åˆå¤§å­¦åŒ—äº¬å¸‚ä¿¡æ¯æœåŠ¡å·¥ç¨‹é‡ç‚¹å®éªŒå®¤</a>;åŒ—äº¬è”åˆå¤§å­¦æœºå™¨äººå­¦é™¢è„‘ä¸è®¤çŸ¥æ™ºèƒ½åŒ—äº¬å®éªŒå®¤;åŒ—äº¬å¼€æ”¾å¤§å­¦;,2024:5.DOI:10.26914/c.cnkihy.2024.047826.</li><li>[126]æ¨æ™¨çº¢,é«˜æŒ¯åˆš,æ¨ç‘é¹,ç­‰.è½»é‡åŒ–ç›®æ ‡æ£€æµ‹æ¨¡å‹ç®—æ³•ç»¼è¿°[J/OL].æœºç”µå·¥ç¨‹æŠ€æœ¯,1-19* [2024-12-25].<a href="http://kns.cnki.net/kcms/detail/44.1522.TH.20241223.0848.002.html" target="_blank" rel="noreferrer">http://kns.cnki.net/kcms/detail/44.1522.TH.20241223.0848.002.html</a>.</li><li>[127]é’Ÿå¸…,ç‹ä¸½è.æ— äººæœºèˆªæ‹å›¾åƒç›®æ ‡æ£€æµ‹æŠ€æœ¯ç ”ç©¶ç»¼è¿°[J/OL].æ¿€å…‰ä¸å…‰ç”µå­å­¦è¿›å±•,1-32* [2024-12-25].<a href="http://kns.cnki.net/kcms/detail/31.1690.TN.20241209.0956.042.html" target="_blank" rel="noreferrer">http://kns.cnki.net/kcms/detail/31.1690.TN.20241209.0956.042.html</a>.</li><li>[128]è‹ä½³,æ¨æ¢¦å‡¡,å¼ æŸæ¨,ç­‰.æ”¹è¿›RT-DETRçš„æ— äººæœºå°ç›®æ ‡æ£€æµ‹ç®—æ³•[J/OL].å¾®ç”µå­å­¦ä¸è®¡ç®—æœº,1-13* [2024-12-25].<a href="http://kns.cnki.net/kcms/detail/61.1123.tn.20241104.1032.004.html" target="_blank" rel="noreferrer">http://kns.cnki.net/kcms/detail/61.1123.tn.20241104.1032.004.html</a>.</li><li>[129]åˆ˜æ— çºª.åŸºäºSSDçš„æ— äººæœºå›¾åƒç›®æ ‡æ£€æµ‹ç®—æ³•ç ”ç©¶[D].å®‰å¾½å·¥ç¨‹å¤§å­¦,2023.DOI:10.27763/d.cnki.gahgc.2023.000482.</li><li>[130]éœçº¢åˆš,å‘¨è ¡,è”¡æ°,ç­‰.åŸºäºå…ˆéªŒçŸ¥è¯†Faster R-CNNçš„è¾“ç”µçº¿è·¯æ— äººæœºå›¾åƒè¯†åˆ«æ–¹æ³•[J].æ™ºæ…§ç”µåŠ›,2024,52(06):108-115.</li><li>[131]å´ä¸€å…¨ï¼Œç«¥åº·.åŸºäºæ·±åº¦å­¦ä¹ çš„æ— äººæœºèˆªæ‹å›¾åƒå°ç›®æ ‡æ£€æµ‹ç ”ç©¶è¿›å±•[J/OL].èˆªç©ºå­¦æŠ¥. <a href="https://link.cnki.net/urlid/11.1929.V.20240923.1021.002" target="_blank" rel="noreferrer">https://link.cnki.net/urlid/11.1929.V.20240923.1021.002</a>.</li><li>[132]å»–ç¦å…°,æ—æ–‡æ ‘,åˆ˜æµ©ç„¶.åŸºäºæ— äººæœºLiDARç‚¹äº‘æ …æ ¼åŒ–å’ŒMask R-CNNç®—æ³•çš„å•æœ¨æ ‘å† åˆ†å‰²[J/OL].å†œä¸šå·¥ç¨‹å­¦æŠ¥,1-9* [2024-12-25].<a href="http://kns.cnki.net/kcms/detail/11.2047.S.20241128.2108.104.html" target="_blank" rel="noreferrer">http://kns.cnki.net/kcms/detail/11.2047.S.20241128.2108.104.html</a>.</li><li>[133]ç½—æ—­ä¸œ,å´ä¸€å…¨,é™ˆé‡‘æ—.æ— äººæœºèˆªæ‹å½±åƒç›®æ ‡æ£€æµ‹ä¸è¯­ä¹‰åˆ†å‰²çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç ”ç©¶è¿›å±•[J].èˆªç©ºå­¦æŠ¥,2024,45(06):241-270.</li><li>[134]åˆ˜ç šèŠ,ç‹é›ªæ¢…,å®‹å»ºè¾‰,ç­‰.åŸºäºæ”¹è¿›RetinaNetçš„è½»é‡åŒ–èˆªæ‹ç›®æ ‡æ£€æµ‹æ–¹æ³•[J].ç«åŠ›ä¸æŒ‡æŒ¥æ§åˆ¶,2024,49(09):76-82+89.</li><li>[135]åˆ˜æ™‹å·,é»å‘é”‹,åˆ˜å®‰æ—­,ç­‰.æ”¹è¿›RetinaNetçš„æ— äººæœºå°ç›®æ ‡æ£€æµ‹[J].ç§‘å­¦æŠ€æœ¯ä¸å·¥ç¨‹,2023,23(01):274-282.</li><li>[136]é»„é›ªèŠ¹,å¼ èƒœ,æœ±å…ˆå¼º,ç­‰.ç”Ÿæˆå¼ä»»åŠ¡ç½‘ï¼šåŸºäºå¤§æ¨¡å‹çš„è‡ªä¸»ä»»åŠ¡è§„åˆ’ä¸æ‰§è¡ŒèŒƒå¼[J/OL].è®¡ç®—æœºç§‘å­¦,1-17* [2024-12-25].<a href="http://kns.cnki.net/kcms/detail/50.1075.TP.20241211.1704.024.html" target="_blank" rel="noreferrer">http://kns.cnki.net/kcms/detail/50.1075.TP.20241211.1704.024.html</a>.</li><li>[137]åˆ˜é›„.ä¸»åŠ¨éšèº«æŠ€æœ¯ä¸­ç›®æ ‡çš„æœ‰æºå¯¹æ¶ˆæ•ˆæœç ”ç©¶[D].ç”µå­ç§‘æŠ€å¤§å­¦,2018.</li><li>[138]é‚±å°å‰‘,éª†åšé›…,ä»˜ç,ç­‰.å›½å†…å¤–åæ— äººæœºæŠ€æœ¯å‘å±•ç»¼è¿°[J].æˆ˜æœ¯å¯¼å¼¹æŠ€æœ¯,2024,(05):63-73+98.DOI:10.16358/j.issn.1009-1300.20240095.</li><li>[139]ææ‡¿å‡¡.æŒ‡æŒ¥æ§åˆ¶ç³»ç»Ÿå¯é æ€§å»ºæ¨¡ä¸è¯„ä¼°æ–¹æ³•ç ”ç©¶[D].ç”µå­ç§‘æŠ€å¤§å­¦,2023.DOI:10.27005/d.cnki.gdzku.2023.005448.</li><li>[140]ç½—å‰‘æ­¦,è‘›æ™ºå›,é™ˆå…‹æ¾.å¤æ‚ç³»ç»Ÿå¯é æ€§è¯„ä¼°æ–¹æ³•ç ”ç©¶ç»¼è¿°[J].ç”µå­äº§å“å¯é æ€§ä¸ç¯å¢ƒè¯•éªŒ,2023,41(06):122-130.</li><li>[141]åˆ˜ä¸€èŒ,ç™½é“­é˜³,å¼ å°å¯,ç­‰.å¤æ‚ç³»ç»Ÿå¯é æ€§[J].å±±ä¸œç§‘å­¦,2024,37(02):74-84.</li><li>[142]é‚¢æ¢¦é›ª,æé¢œ.å…·æœ‰ç›¸ä¾ç«äº‰å¤±æ•ˆçš„å¤æ‚ç³»ç»Ÿå¯é æ€§åˆ†æ[J].è¿ç­¹ä¸ç®¡ç†,2024,33(05):126-131.</li><li>[143]å¼ å¥¡,å­£æ™¨é¾™,æ¯•é›„,ç­‰.è€ƒè™‘è¿è¡Œå‰–é¢çš„å¤æ‚ç³»ç»Ÿå¯é æ€§ç»¼åˆè¯„ä¼°æ–¹æ³•[J].ç”µå­äº§å“å¯é æ€§ä¸ç¯å¢ƒè¯•éªŒ,2024,42(S1):43-47.</li><li>[144]èƒ¡å¯¿æ¾.è‡ªåŠ¨æ§åˆ¶åŸç†(ç¬¬å››ç‰ˆ)[M].ç§‘å­¦å‡ºç‰ˆç¤¾,2001:15-18.</li><li>[145]å¼ æ¢¦è½©,è‹æ²»å®,ç´¢æ—­ä¸œ.ç§»åŠ¨æœºå™¨äººå®šä½æ–¹æ³•ç ”ç©¶ç»¼è¿°[J].è½¦è¾†ä¸åŠ¨åŠ›æŠ€æœ¯,2023,(04):56-62.DOI:10.16599/j.cnki.1009-4687.2023.04.009.</li><li>[146]ç‹è¿å‹.åŸºäºORB-SLAM2ä¸‹å¤šä¿¡æ¯èåˆå››æ—‹ç¿¼æ— äººæœºå®¤å†…å»ºå›¾å®šä½[D].ååŒ—ç”µåŠ›å¤§å­¦(åŒ—äº¬),2024.</li><li>[147]ç‹ç…œ.éç»“æ„åŒ–ç¯å¢ƒä¸‹æ— äººæœºçš„å®šä½ä¸å»ºå›¾æ–¹æ³•ç ”ç©¶[D].å—äº¬ä¿¡æ¯å·¥ç¨‹å¤§å­¦,2024.DOI:10.27248/d.cnki.gnjqc.2024.001605.</li><li>[148]ç¬¦ç¾ç¦.åŸºäºæ— äººæœºçš„åœ°ä¸‹çŸ¿å±±è‡ªä¸»å»ºå›¾æ–¹æ³•ç ”ç©¶[D].è¥¿å—ç§‘æŠ€å¤§å­¦,2024.DOI:10.27415/d.cnki.gxngc.2024.001093.</li><li>[149]å´ç§‰æ…§,è‘£å¿—å²©,ç¿Ÿé¹,ç­‰.æ— äººæœºå¹³é£ä¸‹æ¿€å…‰é›·è¾¾å’ŒåŒç›®è§†è§‰èåˆçš„SLAMå»ºå›¾[J].è®¡ç®—æœºåº”ç”¨ä¸è½¯ä»¶,2024,41(04):192-199.</li><li>[150]é™ˆå¦æ±.åŸºäºç‚¹çº¿ç‰¹å¾èåˆçš„æ— äººæœºå®¤å†…å»ºå›¾æŠ€æœ¯ç ”ç©¶[D].é‡åº†ç†å·¥å¤§å­¦,2024.DOI:10.27753/d.cnki.gcqgx.2024.001214.</li><li>[151]é’±å³°,ç‹ä¸½è‹±,é¹¿ç‚ç‚,ç­‰.åŸºäºå•ç›®è§†è§‰çš„æ— äººæœºä½å§¿ä¼°è®¡ä¸ç¨ å¯†å»ºå›¾æŠ€æœ¯[C]<a href="//xn--jhqtc20b74gx1hvlhs0o4gax5b.xn--2023-384ficw7h23h93bj9dovcd8o5jh98l9y1asvg2jets7jf0rgfgvl7dk1ycma.xn--2qqp8crwwhj0zuodda328ko2y0x3aca627uda4853e" target="_blank" rel="noreferrer">//ä¸­å›½æŒ‡æŒ¥ä¸æ§åˆ¶å­¦ä¼š.2023ç¬¬ä¸ƒå±Šå…¨å›½é›†ç¾¤æ™ºèƒ½ä¸ååŒæ§åˆ¶å¤§ä¼šè®ºæ–‡é›†.æµ·å†›èˆªç©ºå¤§å­¦èˆªç©ºä½œæˆ˜å‹¤åŠ¡å­¦é™¢</a>;æµ·å†›èˆªç©ºå¤§å­¦èˆªç©ºåŸºç¡€å­¦é™¢;,2023:8.DOI:10.26914/c.cnkihy.2023.120072.</li><li>[152]äºç¾¤.æœªçŸ¥ç¯å¢ƒä¸‹å¤šæ—‹ç¿¼æ— äººæœºè‡ªä¸»æ¢ç´¢åŠå»ºå›¾[D].å¤§è¿æµ·äº‹å¤§å­¦,2023.DOI:10.26989/d.cnki.gdlhu.2023.000160.</li><li>[153]å¼ æˆ.åŸºäºå¤šä¼ æ„Ÿå™¨èåˆçš„æ— äººæœºå»ºå›¾ä¸è·¯å¾„è§„åˆ’ç ”ç©¶[D].å‰æ—å¤§å­¦,2023.DOI:10.27162/d.cnki.gjlin.2023.005021.</li><li>[154]ç‹é–.å®¤å†…ç¯å¢ƒæ— äººæœºè§†è§‰åŒæ­¥å®šä½ä¸å»ºå›¾æ–¹æ³•ç ”ç©¶[D].å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦,2022.DOI:10.27061/d.cnki.ghgdu.2022.004176.</li><li>[155]æäºšé£,èµµç‘.åŸå¸‚å¤æ‚ç¯å¢ƒä¸‹å¤šç›®æ ‡æ— äººæœºè·¯å¾„è§„åˆ’ç ”ç©¶[J].å—äº¬èˆªç©ºèˆªå¤©å¤§å­¦å­¦æŠ¥,2024,56(06):1002-1012.DOI:10.16356/j.1005-2615.2024.06.003.</li><li>[156]å­™ä¼Ÿæ˜Œ,ç½—å¿—æµ©,çŸ³å»ºè¿ˆ,ç­‰.æ— äººæœºè¦†ç›–è·¯å¾„è§„åˆ’æ–¹æ³•ç»¼è¿°[J/OL].æ§åˆ¶ç†è®ºä¸åº”ç”¨,1-21* [2024-12-26].<a href="http://kns.cnki.net/kcms/detail/44.1240.TP.20241130.0849.004.html" target="_blank" rel="noreferrer">http://kns.cnki.net/kcms/detail/44.1240.TP.20241130.0849.004.html</a>.</li><li>[157]ç‹å®¶äº®,è‘£æ¥·,é¡¾å…†å†›,ç­‰.å°å‹æ— äººæœºè§†è§‰ä¼ æ„Ÿå™¨é¿éšœæ–¹æ³•ç»¼è¿°[J/OL].è¥¿å®‰ç”µå­ç§‘æŠ€å¤§å­¦å­¦æŠ¥,1-21* [2024-12-26].<a href="https://doi.org/10.19665/j.issn1001-2400.20241008" target="_blank" rel="noreferrer">https://doi.org/10.19665/j.issn1001-2400.20241008</a>.</li><li>[158]ç¥å«±.åŸºäºé£é™©æˆæœ¬çš„æ— äººæœºè·¯å¾„è§„åˆ’ç®—æ³•ç ”ç©¶[D].è¥¿å®‰é‚®ç”µå¤§å­¦,2024.DOI:10.27712/d.cnki.gxayd.2024.000073.</li><li>[159]éƒ­ä¸€å‡¡.åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å››æ—‹ç¿¼æ— äººæœºè·¯å¾„è§„åˆ’ç ”ç©¶[D].è¥¿å®‰å·¥ä¸šå¤§å­¦,2024.DOI:10.27391/d.cnki.gxagu.2024.000504.</li><li>[160]Calonder M. EKF SLAM vs. FastSLAM--A comparison[J]. 2006.</li><li>[162]Grisetti G, Stachniss C, Burgard W. Improved techniques for grid mapping with rao-blackwellized particle filters[J]. IEEE transactions on Robotics, 2007, 23(1): 34-46.</li><li>[163]Kicman P, Silson P, Tsourdos A, et al. Cartographer-A terrain mapping and landmark-based localization system for small robots[M]//Infotech@ Aerospace 2011. 2011: 1513.</li><li>[164]NÃ¼chter A, Bleier M, Schauer J, et al. Improving Google&#39;s Cartographer 3D mapping by continuous-time slam[J]. The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2017, 42: 543-549.</li><li>[165]Zhang J, Singh S. LOAM: Lidar odometry and mapping in real-time[C]//Robotics: Science and systems. 2014, 2(9): 1-9.</li><li>[166]Shan T, Englot B, Meyers D, et al. Lio-sam: Tightly-coupled lidar inertial odometry via smoothing and mapping[C]//2020 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, 2020: 5135-5142.</li><li>[167]Chen X, Milioto A, Palazzolo E, et al. Suma++: Efficient lidar-based semantic slam[C]//2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019: 4530-4537.</li><li>[168]Xu W, Cai Y, He D, et al. Fast-lio2: Fast direct lidar-inertial odometry[J]. IEEE Transactions on Robotics, 2022, 38(4): 2053-2073.</li><li>[169]Qin T, Li P, Shen S. Vins-mono: A robust and versatile monocular visual-inertial state estimator[J]. IEEE transactions on robotics, 2018, 34(4): 1004-1020.</li><li>[170]Chen X, Zhou B, Lin J, et al. Fast 3D sparse topological skeleton graph generation for mobile robot global planning[C]//2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2022: 10283-10289.</li><li>[171]Hu Y, Li T M, Anderson L, et al. Taichi: a language for high-performance computation on spatially sparse data structures[J]. ACM Transactions on Graphics (TOG), 2019, 38(6): 1-16.</li><li>[172]æ¯›å®‡èˆª.æ— äººä¾¦å¯Ÿæœºè£…å¤‡å‘å±•ç°çŠ¶åŠè¶‹åŠ¿[J].å±…èˆ,2017,(29):139.</li><li>[173]æ˜“å¥ä¼.å›½å¤–æ— äººä¾¦å¯Ÿæœºç°çŠ¶åŠå‘å±•è¶‹åŠ¿[J].ä¸­å›½å†›è½¬æ°‘,2023,(04):92-94.</li><li>[174]é‡‘å›½æ ‹,å¢åˆ©æ–Œ,è°·è‰¯è´¤.åŸºäºæ”»é˜²ä¸‹çš„æ— äººä¾¦å¯Ÿæœºçªé˜²æ•ˆèƒ½è¯„ä¼°[J].ç«åŠ›ä¸æŒ‡æŒ¥æ§åˆ¶,2012,37(03):128-132.</li><li>[175]å´”ç‘å¼º,å´ä¼å®¶,æéœ‡,ç­‰.å¾®å‹æ— äººä¾¦å¯Ÿæœºæ°”åŠ¨å¸ƒå±€åŠå…³é”®æŠ€æœ¯[J].é£èˆªå¯¼å¼¹,2011,(08):49-52+58.DOI:10.16338/j.issn.1009-1319.2011.08.010.</li><li>[176]é‡‘å›½æ ‹,å¢åˆ©æ–Œ,ç‹å‰‘é”‹.æ— äººä¾¦å¯Ÿæœºä»»åŠ¡å¯é æ€§è¯„ä¼°æ¨¡å‹[J].æˆ˜æœ¯å¯¼å¼¹æŠ€æœ¯,2005,(02):27-30+51.DOI:10.16358/j.issn.1009-1300.2005.02.006.</li><li>[177]éƒ‘å®æ¥.ç¢³çº¤ç»´å¤åˆææ–™ç”¨äºâ€œç¿¼é¾™â…¡â€æ— äººæœº[J].åˆæˆçº¤ç»´,2017,46(04):42.DOI:10.16090/j.cnki.hcxw.2017.04.014.</li><li>[178]åˆ˜è‰ºæ¶›,å€ªå«å›½.æœ¬ç§‘å±‚æ¬¡èŒä¸šæ•™è‚²æ— äººæœºä¸“ä¸šäººæ‰åŸ¹å…»ä½“ç³»ç ”ç©¶[J].èŒä¸šæŠ€æœ¯,2024,23(12):47-52+72.DOI:10.19552/j.cnki.issn1672-0601.2024.12.008.</li><li>[179]å¼ å‡¤,é©¬æ°¸çº¢,å¼ é£é¾™.åŸºäºå›½å®¶é‡å¤§ç§‘æŠ€é¡¹ç›®çš„ç ”ç©¶ç”ŸåŸ¹å…»æ¨¡å¼ç ”ç©¶â€”â€”ä»¥é•¿é¹°æ— äººæœºä¸ºä¾‹[J].ç ”ç©¶ç”Ÿæ•™è‚²ç ”ç©¶,2024,(06):43-52+117.DOI:10.19834/j.cnki.yjsjy2011.2024.06.05.</li><li>[180]æˆ´æ­£å®—.æŠŠæ¡â€œæœºâ€é‡ä¹˜â€œé£â€è€Œèµ·â€”â€”2023ä¸­å›½(ç»µé˜³)ç§‘æŠ€åŸæ— äººæœºäº§ä¸šå‘å±•å¤§ä¼šé€è§†[N].ä¸­å›½è´¢ç»æŠ¥,2023-10-19(005).DOI:10.28053/n.cnki.nccjb.2023.001885.</li><li>[181]Hart P E, Nilsson N J, Raphael B. A formal basis for the heuristic determination of minimum cost paths[J]. IEEE transactions on Systems Science and Cybernetics, 1968, 4(2): 100-107.</li><li>[182]RiviÃ¨re B, Lathrop J, Chung S J. Monte Carlo tree search with spectral expansion for planning with dynamical systems[J]. Science Robotics, 2024, 9(97): eado1010.</li><li>[183]Dijkstra EW. A note on two problems in connexion with graphs[M]. New York: Krzysztof R. Apt, 2022. 287-290.</li><li>[184]National Aeronautics and Space Administration, NASA Technology Roadmaps TA 8: Science Instruments,Observatories, and Sensor Systems[R], 2015, www.nasa.gov.</li><li>[185]KÃ¼mmerle R, Grisetti G, Strasdat H, et al. g 2 o: A general framework for graph optimization[C]//2011 IEEE international conference on robotics and automation. IEEE, 2011: 3607-3613.</li><li>[186]Grisetti G, KÃ¼mmerle R, Stachniss C, et al. A tutorial on graph-based SLAM[J]. IEEE Intelligent Transportation Systems Magazine, 2010, 2(4): 31-43.</li><li>[187]Rublee E, Rabaud V, Konolige K, et al. ORB: An efficient alternative to SIFT or SURF[C]//2011 International conference on computer vision. Ieee, 2011: 2564-2571.</li><li>[188]Galvez-Lopez D, Tardos J D. Real-time loop detection with bags of binary words[C]// IEEE/RSJ International Conference on Intelligent Robots &amp; Systems. 2011:51-58.</li><li>[189]Ramsey C W, Kingston Z, Thomason W, et al. Collision-Affording Point Trees: SIMD-Amenable Nearest Neighbors for Fast Collision Checking[J]. arXiv preprint arXiv:2406.02807, 2024.</li><li>[190]Ren, Y. F., et al. Safety-assured high-speed navigation for MAVs [J]. Sci. Robot., 2025, 10: eado6187. DOI:10.1126/scirobotics.ado6187.</li><li>[191]LÃ¶schne F, BÃ¶ttcher T, Jeske S R, Bender J. Weighted Laplacian smoothing for surface reconstruction of particle-based fluids[C]//Vision, Modeling, and Visualization. The Eurographics Association, 2023: 978-3-03868-232-5. DOI:10.2312/vmv.20231245.</li><li>[192]Kazhdan M, Bolitho M, Hoppe H. Poisson surface reconstruction[C]//Proceedings of the fourth Eurographics symposium on Geometry processing. 2006, 7(4).</li><li>[193]Kazhdan M, Hoppe H. Screened poisson surface reconstruction[J]. ACM Transactions on Graphics (ToG), 2013, 32(3): 1-13.</li><li>[194]Piegl L, Tiller W. The NURBS book[M]. Springer Science &amp; Business Media, 2012.</li><li>[195]Wang Y, Barrios Romero D, Lemire D, Jin L. Modern Non-Cryptographic Hash Function and Pseudorandom Number Generator. [Online]. Available: <a href="https://github.com/wangyi-fudan/wyhash/blob/master/Modern%20Non-Cryptographic%20Hash%20Function%20and%20Pseudorandom%20Number%20Generator.pdf" target="_blank" rel="noreferrer">https://github.com/wangyi-fudan/wyhash/blob/master/Modern Non-Cryptographic Hash Function and Pseudorandom Number Generator.pdf</a>. [è®¿é—®: 2025-02-10].</li><li>[196]Campos C, Elvira R, RodrÃ­guez J J G, et al. Orb-slam3: An accurate open-source library for visual, visualâ€“inertial, and multimap slam[J]. IEEE Transactions on Robotics, 2021, 37(6): 1874-1890.</li><li>[197]Dong Y, Gong W, Li Q, et al. ORB-SLAM3AB: Augmenting ORB-SLAM3 to Counteract Bumps with Optical Flow Inter-frame Matching[J]. arXiv preprint arXiv:2411.18174, 2024.</li><li>[198]Raoui Y, Weber C, Wermter S. NeoSLAM: Neural object SLAM for loop closure and navigation[C]//International Conference on Artificial Neural Networks. Cham: Springer Nature Switzerland, 2022: 443-455.</li><li>[199]Fritzson P, Pop A, Abdelhak K, et al. The OpenModelica integrated environment for modeling, simulation, and model-based development[C]. Mic, 2022.</li><li>[200]Holzmann G J. The power of 10: Rules for developing safety-critical code[J]. Computer, 2006, 39(6): 95-99.</li><li>[201]Chi P, Wei J, Zhao J, et al. Autonomous UAV Exploration in Unknown Environments Using Octomap[C]//International Conference on Guidance, Navigation and Control. Singapore: Springer Nature Singapore, 2024: 357-365.</li><li>[202]WidÃ©n L, Wiman E. Autonomous 3D exploration with dynamic obstacles: Towards Intelligent Navigation and Collision Avoidance for Autonomous 3D Exploration with dynamic obstacles[J]. 2023.</li><li>[203]Zhao Y, Yan L, Xie H, et al. Autonomous exploration method for fast unknown environment mapping by using UAV equipped with limited FOV sensor[J]. IEEE Transactions on Industrial Electronics, 2023, 71(5): 4933-4943.</li><li>[204]Lu G, Ren Y, Zhu F, et al. Autonomous Tail-Sitter Flights in Unknown Environments[J]. IEEE Transactions on Robotics, 2025.</li><li>[205]Eldemiry A, Zou Y, Li Y, et al. Autonomous exploration of unknown indoor environments for high-quality mapping using feature-based RGB-D SLAM[J]. Sensors, 2022, 22(14): 5117.</li><li>[206]Coelho F O, Pinto M F, Biundini I Z, et al. Autonomous uav exploration and mapping in uncharted terrain through boundary-driven strategy[J]. IEEE Access, 2024.</li><li>[207]Deng W, Qi M, Ma H. Global-Local Tree Search in VLMs for 3D Indoor Scene Generation[J]. arXiv e-prints, 2025: arXiv: 2503.18476.</li><li>[208]Wang Y, Chen S Y, Zhou Z, et al. ROOT: VLM based System for Indoor Scene Understanding and Beyond[J]. arXiv preprint arXiv:2411.15714, 2024.</li><li>[209]Edstedt J, BÃ¶kman G, WadenbÃ¤ck M, et al. DaD: Distilled Reinforcement Learning for Diverse Keypoint Detection[J]. arXiv preprint arXiv:2503.07347, 2025.</li><li>[210]Edstedt J, BÃ¶kman G, Zhao Z. DeDoDe v2: Analyzing and Improving the DeDoDe Keypoint Detector[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 4245-4253.</li><li>[211]Yin Z, Cheng C. Navigation with VLM framework: Go to Any Language[J]. arXiv preprint arXiv:2410.02787, 2024.</li><li>[212]Zhang Z, Gupta A, Jiang H, et al. Neuflow v2: High-efficiency optical flow estimation on edge devices[J]. arXiv preprint arXiv:2408.10161, 2024.</li><li>[213]PaddlePaddle. PP-OCRv4æŠ€æœ¯æŠ¥å‘Š[EB/OL]. PaddleOCR, 2023* [2025-04-25]. <a href="https://paddlepaddle.github.io/PaddleOCR/main/ppocr/blog/PP-OCRv4_introduction.html" target="_blank" rel="noreferrer">https://paddlepaddle.github.io/PaddleOCR/main/ppocr/blog/PP-OCRv4_introduction.html</a>.</li><li>[214]Baviskar D, Ahirrao S, Potdar V, et al. Efficient automated processing of the unstructured documents using artificial intelligence: A systematic literature review and future directions[J]. IEEE Access, 2021, 9: 72894-72936.</li><li>[215]Ravi N, Gabeur V, Hu Y T, et al. Sam 2: Segment anything in images and videos[J]. arXiv preprint arXiv:2408.00714, 2024.</li><li>[216]Kim T, Min B C. Semantic Layering in Room Segmentation via LLMs[C]//2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2024: 9831-9838.</li><li>[217]Wang J, Chen M, Karaev N, et al. VGGT: Visual Geometry Grounded Transformer[J]. arXiv preprint arXiv:2503.11651, 2025.</li><li>[218]Xiao Z, Li S. SL-SLAM: A robust visual-inertial SLAM based deep feature extraction and matching[J]. arXiv e-prints, 2024: arXiv: 2405.03413.</li><li>[219]Wu W, Mao S, Zhang Y, et al. Mind&#39;s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models[C]//The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024.</li></ul>',2)]))}const g=e(t,[["render",o]]);export{p as __pageData,g as default};
